{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ML AI test\\AI Factory\\YOLO_Test_from_TF\n"
     ]
    }
   ],
   "source": [
    "cd ./YOLO_Test_from_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n",
    "vid_formats = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes\n",
    "\n",
    "def dataset_letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "class LoadImages:  # for inference\n",
    "    def __init__(self, path, img_size=640, stride=32, auto=True):\n",
    "        p = str(Path(path).absolute())  # os-agnostic absolute path\n",
    "        if '*' in p:\n",
    "            files = sorted(glob.glob(p, recursive=True))  # glob\n",
    "        elif os.path.isdir(p):\n",
    "            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n",
    "        elif os.path.isfile(p):\n",
    "            files = [p]  # files\n",
    "        else:\n",
    "            raise Exception(f'ERROR: {p} does not exist')\n",
    "\n",
    "        images = [x for x in files if x.split('.')[-1].lower() in img_formats]\n",
    "        videos = [x for x in files if x.split('.')[-1].lower() in vid_formats]\n",
    "        ni, nv = len(images), len(videos)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.files = images + videos\n",
    "        self.nf = ni + nv  # number of files\n",
    "        self.video_flag = [False] * ni + [True] * nv\n",
    "        self.mode = 'image'\n",
    "        self.auto = auto\n",
    "        if any(videos):\n",
    "            self.new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
    "                            f'Supported formats are:\\nimages: {img_formats}\\nvideos: {vid_formats}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.nf:\n",
    "            raise StopIteration\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        if self.video_flag[self.count]:\n",
    "            # Read video\n",
    "            self.mode = 'video'\n",
    "            ret_val, img0 = self.cap.read()\n",
    "            if not ret_val:\n",
    "                self.count += 1\n",
    "                self.cap.release()\n",
    "                if self.count == self.nf:  # last video\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    path = self.files[self.count]\n",
    "                    self.new_video(path)\n",
    "                    ret_val, img0 = self.cap.read()\n",
    "\n",
    "            self.frame += 1\n",
    "            #print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: ', end='')\n",
    "\n",
    "        else:\n",
    "            # Read image\n",
    "            self.count += 1\n",
    "            img0 = cv2.imread(path)  # BGR\n",
    "            assert img0 is not None, 'Image Not Found ' + path\n",
    "            #print(f'image {self.count}/{self.nf} {path}: ', end='')\n",
    "\n",
    "        # Padded resize\n",
    "        img = dataset_letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        return path, img, img0, self.cap\n",
    "\n",
    "    def new_video(self, path):\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nf  # number of files\n",
    "\n",
    "class Colors:\n",
    "    # Ultralytics color palette https://ultralytics.com/\n",
    "    def __init__(self):\n",
    "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
    "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
    "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
    "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
    "        self.n = len(self.palette)\n",
    "\n",
    "    def __call__(self, i, bgr=False):\n",
    "        c = self.palette[int(i) % self.n]\n",
    "        return (c[2], c[1], c[0]) if bgr else c\n",
    "\n",
    "    @staticmethod\n",
    "    def hex2rgb(h):  # rgb order (PIL)\n",
    "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
    "\n",
    "def plot_one_box(x, im, color=(128, 128, 128), label=None, line_thickness=3):\n",
    "    # Plots one bounding box on image 'im' using OpenCV\n",
    "    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n",
    "    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def letterbox(img):\n",
    "    new_shape=(640, 640)\n",
    "    color=(114, 114, 114)\n",
    "    auto=True\n",
    "    scaleFill=False\n",
    "    scaleup=True\n",
    "    stride=32\n",
    "\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "def clip_coords(boxes, img_shape):\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n",
    "    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n",
    "    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n",
    "    boxes[:, 3].clamp_(0, img_shape[0])  # y2 \n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:   # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "    clip_coords(coords, img0_shape)\n",
    "    return coords\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=(), max_det=300):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            l = labels[xi]\n",
    "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
    "            v[:, :4] = l[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def detect(opt):\n",
    "    source, weights, imgsz = opt.source, opt.weights, opt.img_size\n",
    "    save_dir = Path('output')\n",
    "    weights = weights[0] \n",
    "    backend = 'tflite'\n",
    "    save_img = True  # save inference images\n",
    "    stride = None\n",
    "    with open('coco.yaml') as f:\n",
    "        names = yaml.load(f, Loader=yaml.FullLoader)['names']  # class names (assume COCO)\n",
    "\n",
    "    # Load TFLite model and allocate tensors\n",
    "    interpreter = tf.lite.Interpreter(model_path=weights)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Data preprocessing\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=backend == 'pytorch')\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img)\n",
    "        img =  img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Run inference\n",
    "        '''input_data = np.expand_dims(img,axis=0)'''\n",
    "        input_data = img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)   #1,320,320,3\n",
    "        interpreter.invoke()\n",
    "        pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "        # Denormalize xywh\n",
    "        pred[..., 0] *= imgsz[1]  # x\n",
    "        pred[..., 1] *= imgsz[0]  # y\n",
    "        pred[..., 2] *= imgsz[1]  # w\n",
    "        pred[..., 3] *= imgsz[0]  # h\n",
    "        pred = torch.tensor(pred)\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, opt.classes, opt.agnostic_nms,\n",
    "                                    max_det=opt.max_det)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            p, s, im0 = path, '', im0s\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string    1,3,320,320\n",
    "        \n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_img : # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if opt.hide_labels else (names[c] if opt.hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=opt.line_thickness)\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--weights', nargs='+', type=str, default='yolov5s_dynm_range.tflite', help='model.pt path(s)')\n",
    "#     parser.add_argument('--source', type=str, default='bus.jpg', help='source')  # file.jpg, 0 for webcam\n",
    "#     parser.add_argument('--img-size', nargs='+', type=int, default=[320, 320], help='image size')  # height, width\n",
    "#     parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n",
    "#     parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
    "#     parser.add_argument('--max-det', type=int, default=1000, help='maximum number of detections per image')\n",
    "#     parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
    "#     parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "#     parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "#     parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "#     parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
    "#     parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
    "#     parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
    "#     opt = parser.parse_args()\n",
    "#     opt.img_size *= 2 if len(opt.img_size) == 1 else 1  # expand\n",
    "    \n",
    "#     detect(opt=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExInput:\n",
    "    weights = 'D:\\ML AI test\\AI Factory\\YOLO_Test_from_TF\\lite-model_yolo-v5-tflite_tflite_model_1.tflite'\n",
    "    source = 'bus.jpg'\n",
    "    img_size = [320, 320]\n",
    "    conf_thres = 0.25\n",
    "    iou_thres = 0.45\n",
    "    max_det = 1000\n",
    "    classes = None\n",
    "    agnostic_nms = False\n",
    "    augment = False\n",
    "    name = 'exp'\n",
    "    line_thickness = 3\n",
    "    hide_labels = False\n",
    "    hide_conf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not open '.'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6f3250cfe331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexInput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\miron\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-4df6c423d7b9>\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;31m# Load TFLite model and allocate tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m     \u001b[0minterpreter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m     \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\miron\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[0;32m    456\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[0;32m    457\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[0;32m    459\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to open {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not open '.'."
     ]
    }
   ],
   "source": [
    "exInput = ExInput()\n",
    "detect(opt=exInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML AI test\\\\AI Factory\\\\YOLO_Test_from_TF'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
